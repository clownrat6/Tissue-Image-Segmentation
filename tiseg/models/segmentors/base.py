from abc import ABCMeta, abstractmethod
from collections import OrderedDict

import torch
import torch.nn.functional as F
import torch.distributed as dist
from mmcv.runner import BaseModule

from tiseg.utils import resize


def _parse_losses(losses):
    """Parse the raw outputs (losses) of the network.

    Args:
        losses (dict): Raw output of the network, which usually contain
            losses and other necessary information.

    Returns:
        tuple[Tensor, dict]: (loss, log_vars), loss is the loss tensor
            which may be a weighted sum of all losses, log_vars contains
            all the variables to be sent to the logger.
    """
    log_vars = OrderedDict()
    for loss_name, loss_value in losses.items():
        if isinstance(loss_value, torch.Tensor):
            log_vars[loss_name] = loss_value.mean()
        elif isinstance(loss_value, list):
            log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)
        else:
            raise TypeError(f'{loss_name} is not a tensor or list of tensors')

    loss = sum(_value for _key, _value in log_vars.items() if 'loss' in _key)

    log_vars['loss'] = loss
    for loss_name, loss_value in log_vars.items():
        # reduce loss when distributed training
        if dist.is_available() and dist.is_initialized():
            loss_value = loss_value.data.clone()
            dist.all_reduce(loss_value.div_(dist.get_world_size()))
        log_vars[loss_name] = loss_value.item()

    return loss, log_vars


class BaseSegmentor(BaseModule, metaclass=ABCMeta):
    """Segmentor supports multiplt data & multiple labels.

    For example (CDNet):
        Data: image;
        Label: sem_gt, dir_gt, point_gt;
    """

    @abstractmethod
    def calculate(self, img):
        """Calculate the semantic logit result."""
        pass

    @abstractmethod
    def forward(self, **kwargs):
        """When training, the module is required to return loss dict. When
        evaluation, the module is required to return instance-level or
        semantic-level results."""
        pass

    def train_step(self, data_batch, optimizer, **kwargs):
        """The iteration step during training.

        This method defines an iteration step during training, except for the
        back propagation and optimizer updating, which are done in an optimizer
        hook. Note that in some complicated cases or models, the whole process
        including back propagation and optimizer updating is also defined in
        this method, such as GAN.

        Args:
            data_batch (dict): The output of dataloader.
            optimizer (:obj:`torch.optim.Optimizer` | dict): The optimizer of
                runner is passed to ``train_step()``. This argument is unused
                and reserved.

        Returns:
            dict: It should contain at least 3 keys: ``loss``, ``log_vars``,
                ``num_samples``.
                ``loss`` is a tensor for back propagation, which can be a
                weighted sum of multiple losses.
                ``log_vars`` contains all the variables to be sent to the
                logger.
                ``num_samples`` indicates the batch size (when the model is
                DDP, it means the batch size on each GPU), which is used for
                averaging the logs.
        """
        losses = self(**data_batch)
        loss, log_vars = _parse_losses(losses)

        outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(data_batch['metas']))

        return outputs

    def val_step(self, data_batch, **kwargs):
        """The iteration step during validation.

        This method shares the same signature as :func:`train_step`, but used
        during val epochs. Note that the evaluation after training epochs is
        not implemented with this method, but an evaluation hook.
        """
        output = self(**data_batch, **kwargs)
        return output

    def split_axes(self, window_size, overlap_size, height, width):
        """Calculate patch coordinates of split inference."""
        ws = window_size
        os = overlap_size

        i_axes = [0]
        j_axes = [0]
        cur = 0
        edge_base = ws - os // 2
        middle_base = ws - os
        while True:
            if cur == 0:
                cur += edge_base
            else:
                cur += middle_base

            i_axes.append(cur)
            j_axes.append(cur)

            if cur + edge_base == height:
                i_axes.append(cur + edge_base)
            if cur + edge_base == width:
                j_axes.append(cur + edge_base)

            if i_axes[-1] == height and j_axes[-1] == width:
                break

        return i_axes, j_axes

    def split_inference(self, img, meta, rescale):
        """Split inference: split img into several patches for network inference. Then merge them with a overlap.

        How does this function split the img? For example:
            img (torch.Tensor): (1, 3, 1000, 1000) -> H = 1000, W = 1000
            crop_size (int): 256
            overlap_size (int): 80

        H dimension:
            patch 0: (0, 216)
            patch 1: (216, 392)  overlap 0-1: (176, 256)
            patch 2: (392, 568)  overlap 1-2: (352, 432)
            patch 3: (568, 744)  overlap 2-3: (528, 608)
            patch 4: (744, 920)  overlap 3-4: (704, 784)
            patch 5: (920, 1136) overlap 4-5: (880, 960)
        W dimension is same.
        """
        ws = self.test_cfg.crop_size[0]
        os = self.test_cfg.overlap_size[0]

        B, C, H, W = img.shape

        # zero pad for border patches
        pad_h = 0
        pad_w = 0
        if H - ws > 0:
            pad_h = (ws - os) - (H - ws) % (ws - os)

        if W - ws > 0:
            pad_w = (ws - os) - (W - ws) % (ws - os)

        H1 = pad_h + H
        W1 = pad_w + W

        img_canvas = torch.zeros((B, C, H1, W1), dtype=img.dtype, device=img.device)
        img_canvas.fill_(0)
        img_canvas[:, :, pad_h // 2:pad_h // 2 + H, pad_w // 2:pad_w // 2 + W] = img

        _, _, H1, W1 = img_canvas.shape
        sem_output = torch.zeros((B, self.num_classes, H1, W1))

        i_axes, j_axes = self.split_axes(ws, os, H1, W1)

        for i in range(len(i_axes) - 1):
            for j in range(len(j_axes) - 1):
                r_patch_s = i_axes[i] if i == 0 else i_axes[i] - os // 2
                r_patch_e = r_patch_s + ws
                c_patch_s = j_axes[j] if j == 0 else j_axes[j] - os // 2
                c_patch_e = c_patch_s + ws
                img_patch = img_canvas[:, :, r_patch_s:r_patch_e, c_patch_s:c_patch_e]
                sem_patch = self.calculate(img_patch)

                # patch overlap remove
                r_valid_s = i_axes[i] - r_patch_s
                r_valid_e = i_axes[i + 1] - r_patch_s
                c_valid_s = j_axes[j] - c_patch_s
                c_valid_e = j_axes[j + 1] - c_patch_s
                sem_patch = sem_patch[:, :, r_valid_s:r_valid_e, c_valid_s:c_valid_e]
                sem_output[:, :, i_axes[i]:i_axes[i + 1], j_axes[j]:j_axes[j + 1]] = sem_patch

        sem_output = sem_output[:, :, (H1 - H) // 2:(H1 - H) // 2 + H, (W1 - W) // 2:(W1 - W) // 2 + W]
        if rescale:
            sem_output = resize(sem_output, size=meta['ori_hw'], mode='bilinear', align_corners=False)
        return sem_output

    # NOTE: slide_inference isn't practical for special seg task.
    # def slide_inference(self, img, meta, rescale):
    #     """Inference by sliding-window with overlap.

    #     If h_crop > h_img or w_crop > w_img, the small patch will be used to
    #     decode without padding.
    #     """
    #     h_stride, w_stride = self.test_cfg.stride
    #     h_crop, w_crop = self.test_cfg.crop_size
    #     batch_size, _, h_img, w_img = img.size()
    #     num_classes = self.num_classes
    #     h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1
    #     w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1
    #     preds = img.new_zeros((batch_size, num_classes, h_img, w_img))
    #     count_mat = img.new_zeros((batch_size, 1, h_img, w_img))
    #     for h_idx in range(h_grids):
    #         for w_idx in range(w_grids):
    #             y1 = h_idx * h_stride
    #             x1 = w_idx * w_stride
    #             y2 = min(y1 + h_crop, h_img)
    #             x2 = min(x1 + w_crop, w_img)
    #             y1 = max(y2 - h_crop, 0)
    #             x1 = max(x2 - w_crop, 0)
    #             crop_img = img[:, :, y1:y2, x1:x2]
    #             crop_seg_logit = self.calculate(crop_img)
    #             preds += F.pad(crop_seg_logit, (int(x1), int(preds.shape[3] - x2), int(y1), int(preds.shape[2] - y2)))

    #             count_mat[:, :, y1:y2, x1:x2] += 1
    #     assert (count_mat == 0).sum() == 0
    #     preds = preds / count_mat
    #     if rescale:
    #         preds = resize(preds, size=meta['ori_hw'], mode='bilinear', align_corners=False)
    #     return preds

    # NOTE: old style split inference
    # def split_inference(self, img, meta, rescale):
    #     """using half-and-half strategy to slide inference."""
    #     window_size = self.test_cfg.crop_size[0]
    #     overlap_size = self.test_cfg.overlap_size[0]

    #     N, C, H, W = img.shape

    #     input = img

    #     # zero pad for border patches
    #     pad_h = 0
    #     if H - window_size > 0:
    #         pad_h = (window_size - overlap_size) - (H - window_size) % (window_size - overlap_size)
    #         tmp = torch.zeros((N, C, pad_h, W)).to(img.device)
    #         input = torch.cat((input, tmp), dim=2)

    #     if W - window_size > 0:
    #         pad_w = (window_size - overlap_size) - (W - window_size) % (window_size - overlap_size)
    #         tmp = torch.zeros((N, C, H + pad_h, pad_w)).to(img.device)
    #         input = torch.cat((input, tmp), dim=3)

    #     _, C1, H1, W1 = input.size()

    #     output = torch.zeros((input.size(0), 3, H1, W1)).to(img.device)
    #     for i in range(0, H1 - overlap_size, window_size - overlap_size):
    #         r_end = i + window_size if i + window_size < H1 else H1
    #         ind1_s = i + overlap_size // 2 if i > 0 else 0
    #         ind1_e = (i + window_size - overlap_size // 2 if i + window_size < H1 else H1)
    #         for j in range(0, W1 - overlap_size, window_size - overlap_size):
    #             c_end = j + window_size if j + window_size < W1 else W1

    #             input_patch = input[:, :, i:r_end, j:c_end]
    #             input_var = input_patch
    #             output_patch = self.calculate(input_var)

    #             ind2_s = j + overlap_size // 2 if j > 0 else 0
    #             ind2_e = (j + window_size - overlap_size // 2 if j + window_size < W1 else W1)
    #             output[:, :, ind1_s:ind1_e, ind2_s:ind2_e] = output_patch[:, :, ind1_s - i:ind1_e - i,
    #                                                                       ind2_s - j:ind2_e - j]

    #     output = output[:, :, :H, :W]
    #     if rescale:
    #         output = resize(output, size=meta['ori_hw'], mode='bilinear', align_corners=False)
    #     return output

    def whole_inference(self, img, meta, rescale):
        """Inference with full image."""

        sem_logit = self.calculate(img)
        if rescale:
            sem_logit = resize(sem_logit, size=meta['ori_hw'], mode='bilinear', align_corners=False)

        return sem_logit

    def inference(self, img, meta, rescale):
        """Inference with split/whole style.

        Args:
            img (Tensor): The input image of shape (N, 3, H, W).
            meta (dict): Image info dict.
            rescale (bool): Whether rescale back to original shape.

        Returns:
            Tensor: The output segmentation map.
        """
        assert self.test_cfg.mode in ['split', 'whole']

        self.rotate_degrees = self.test_cfg.get('rotate_degrees', [0])
        self.flip_directions = self.test_cfg.get('flip_directions', ['none'])
        sem_logit_list = []
        img_ = img
        for rotate_degree in self.rotate_degrees:
            for flip_direction in self.flip_directions:
                img = self.tta_transform(img_, rotate_degree, flip_direction)

                # inference patch or whole img
                if self.test_cfg.mode == 'split':
                    sem_logit = self.split_inference(img, meta, rescale)
                else:
                    sem_logit = self.whole_inference(img, meta, rescale)

                sem_logit = self.reverse_tta_transform(sem_logit, rotate_degree, flip_direction)
                sem_logit = F.softmax(sem_logit, dim=1)

                sem_logit_list.append(sem_logit)

        sem_logit = sum(sem_logit_list) / len(sem_logit_list)

        return sem_logit

    @classmethod
    def tta_transform(self, img, rotate_degree, flip_direction):
        """TTA transform function.

        Support transform:
            rotation: 0, 90, 180, 270
            flip: horizontal, vertical, diagonal
        """
        rotate_num = (rotate_degree // 90) % 4
        img = torch.rot90(img, k=rotate_num, dims=(-2, -1))

        if flip_direction == 'horizontal':
            img = torch.flip(img, dims=[-1])
        if flip_direction == 'vertical':
            img = torch.flip(img, dims=[-2])
        if flip_direction == 'diagonal':
            img = torch.flip(img, dims=[-2, -1])

        return img

    @classmethod
    def reverse_tta_transform(self, img, rotate_degree, flip_direction):
        """reverse TTA transform function.

        Support transform:
            rotation: 0, 90, 180, 270
            flip: horizontal, vertical, diagonal
        """
        rotate_num = 4 - (rotate_degree // 90) % 4
        if flip_direction == 'horizontal':
            img = torch.flip(img, dims=[-1])
        if flip_direction == 'vertical':
            img = torch.flip(img, dims=[-2])
        if flip_direction == 'diagonal':
            img = torch.flip(img, dims=[-2, -1])

        img = torch.rot90(img, k=rotate_num, dims=(-2, -1))

        return img
